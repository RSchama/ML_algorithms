{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae8d8c22",
   "metadata": {},
   "source": [
    "# Logistic Regression with Gradient Ascent from Scratch #\n",
    "\n",
    "This notebook contains my implementation of the logistic regression with a gradient Ascent. I have also left the simple code `.py` file in the `code` folder, hoping that it it might be easier to imporve the code with `git`. So, I expect that at some point this notebook might be deprecated, and will be be used mostly as a tutorial.  \n",
    "\n",
    "Logistic regression is analogous to multiple linear regression, except the outcome is binary (0/1). There is also no closed form solution, and the model is fit using Maximum Likelihood Estimation (MLE). With MLE we are trying to find the model that most likely produced the data we see. I will talk more about the steps we need to take when we start building our algorithm.   \n",
    "\n",
    "Logistic Regression is used for classification, which is a very important task in Machine Learning. It is part of the Supervised Learning techniques, where we use part of the known data to to learn the best model and parameters that fit our data. The known data comes as pairs of inputs ($X$) and outputs ($y$) to be able to train a model, and it is used in many applications. \n",
    "\n",
    "So, we have an input with its attribute set (variables) and we use the classification algorithm to learn how to assign class labels to these examples.  \n",
    "\n",
    "Classification is a form of prediction where the goal is to predict the class labels, whether if it is binary or multiclass. In this example we will use the UCI Machine Learning Repository [Gisette](https://archive.ics.uci.edu/ml/datasets/Gisette) dataset. This dataset is a handwritten digit problem, where we want to separate the highly confusible digits '4' and '9', and it was constructed from the well known MNIST dataset. Since we only want to discriminate between '4' and '9', this is a binary classification problem.  \n",
    "\n",
    "***\n",
    "\n",
    "### Train and validation sets ###\n",
    "\n",
    "In order to be able to see how good our model is we need to have samples for training it and another set of samples, not \"seen\" by the model, to infer how well it generalizes to unseen data. Normally we would use `sklearn` `train_test_split` to split our data into train and test sets (or use cross-validation). Nevertheless, Gisette, as part of a competition dataset, already has separate sets for training the model (called `gisette_train.data` and `gisette_train.labels`) and a separate set for testing, or validating it (called `gisette_valid.data` and `gisette_valid.labels`).  \n",
    "\n",
    "***\n",
    "\n",
    "### Libraries needed ###\n",
    "\n",
    "This tutorial makes use of varying python libraries at each step. All libraries used are imported in this first step for clarity purposes and to show what requirements are needed at the beginning. Let's import the python libraries we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a16d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.matlib\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7affde",
   "metadata": {},
   "source": [
    "***\n",
    "### Data ###\n",
    "\n",
    "Gisette is composed of a training set with a $\\mathbb{R}^{6000 \\times5000}$ data matrix and $\\mathbb{R}^{6000}$ vector of target labels and a validation set with a $\\mathbb{R}^{1000 \\times 5000}$ data matrix and $\\mathbb{R}^{1000}$ vector of target labels.  \n",
    "\n",
    "We will have two data matrices: X_train and X_test, which later we will call `x` and `xtest` to make writting the code easier. Each will have their own labels: y_train and y_test, which we will call `y` and `ytest` later on.   \n",
    "\n",
    "Let's load our data! The code below will open the files from the UCI website directly and not download them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d52382b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the data\n",
    "url1 = \"https://archive.ics.uci.edu/ml/machine-learning-databases/gisette/GISETTE/gisette_train.data\"\n",
    "X_train = np.asarray(pd.read_csv(url1 , header=None, delim_whitespace=True))\n",
    "url2 = 'https://archive.ics.uci.edu/ml/machine-learning-databases/gisette/GISETTE/gisette_train.labels'\n",
    "y_train = np.asarray(pd.read_csv(url2 , header=None, delim_whitespace=True)).squeeze()\n",
    "url3 = 'https://archive.ics.uci.edu/ml/machine-learning-databases/gisette/GISETTE/gisette_valid.data'\n",
    "X_test = np.asarray(pd.read_csv(url3 , header=None, delim_whitespace=True))\n",
    "url4 = 'https://archive.ics.uci.edu/ml/machine-learning-databases/gisette/gisette_valid.labels'\n",
    "y_test = np.asarray(pd.read_csv(url4 , header=None, delim_whitespace=True)).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ccfe4f",
   "metadata": {},
   "source": [
    "I find it is always important to print the size of our data matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f2e9c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 5000) (1000, 5000) (6000,) (1000,)\n"
     ]
    }
   ],
   "source": [
    "#printing the size of each ndarray\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f87eda",
   "metadata": {},
   "source": [
    "***\n",
    "### Scaling and normalizing data ###\n",
    "\n",
    "Normaly it is good practice to check our data variables' distribution and scale. When we have variables with extremely different scales the ones with higher values will have more weight and might bias the analysis. So we can normalize the data to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values. We would have all variables now with ranges between 0 and 1. Or, when we have a variable with normal disribution, we can standardize the data by subtracting the mean and dividing by the standard deviation in order to have all attributes to have mean 0 and 1 standard deviation. This is useful when your data has varying scales and the algorithm you are using does make assumptions about your data having a Gaussian distribution. In fact, many Machine Learning agorithms assume that all features are centered around 0 and have variance in the same order.\n",
    "\n",
    "Although stardization os not required for logistic regression, it certainly helps convergence, especially because the algorithm does assume a linear distribution of variables.  \n",
    "\n",
    "We will use `sklearn` [preprocessing](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to normalize the data matrices: Standardize features by removing the mean and scaling to unit variance. It is important to remember to `fit` only to the train data in order to prevent data leakage problems. Mean and standard deviation are then stored to be used on later data using `tranform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9fcaa0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "x=scaler.fit_transform(X_train)\n",
    "xtest=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afa3f66",
   "metadata": {},
   "source": [
    "***\n",
    "### Preparing our data matrix ###\n",
    "\n",
    "In order to be able to consider the $\\beta_0$, or bias term, and keep it when we update our parameters, we need to input a column of ones (neutral value) to our data matrices. The code below creates a `numpy` 1-dimensional array of ones the same size as the number of rows in our datasets. We use this vector of ones of the correct size and insert it into our data matrix as the first column (index = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e62cbabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding a column of ones do dataset, as a numpy array\n",
    "#creating a vector of N 1s for train data 6000 and test 1000\n",
    "ones=np.ones((6000,), dtype=int)\n",
    "x = np.insert(x, 0, ones, axis=1)\n",
    "#For the test set as well\n",
    "ones=np.ones((1000,), dtype=int)\n",
    "xtest = np.insert(xtest, 0, ones, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ec30ed",
   "metadata": {},
   "source": [
    "***\n",
    "### Preparing our labels vector ###\n",
    "\n",
    "The logistic function that we use to transform our MLE of the probability _p_ that the label is \"1\" ensures that this number stays between 0 and 1. In order to fit our model we need to compare the resutls that we get at each iteration to the true label vector for our data, so we can correct the parameters and keep going in the right direction with our gradient ascent.  \n",
    "\n",
    "For this reason our label vectors (`y_train` and `y_test`) need to be in a 0 or 1 format. For some datasets that I have encoutered, this label vector is -1 and 1, and we need to convert the -1 to 0. The code below does just that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27d783c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing -1 to 0 in labels (y)\n",
    "y=np.where(y_train == -1, 0, y_train)\n",
    "ytest=np.where(y_test == -1, 0, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae33f6f3",
   "metadata": {},
   "source": [
    "***\n",
    "### Defining the functions we will need ###\n",
    "\n",
    "For this algorithm we will need three different functions, two would have suficed but I wanted the derivative term calculated separately. The `likelihood` function calculates the lieklihood of the model with the parameters used ($\\beta_0, \\beta_1, ...,\\beta_p $), here a vector called $w$. Where $p$ is the number of features in our dataset (here 5000). If we have $j = 1, ..., N$ number of observations;\n",
    "\n",
    "We calculate the log odds for the logistic regression as:\n",
    "\n",
    "$$log\\Bigg(\\frac{P(y_j = 1)}{P(y_j = 0)}\\Bigg) = \\beta_0 + \\beta_1x_{j1} + ... + \\beta_{5000}x_{j5000}$$\n",
    "\n",
    "We start with a parameter vector $w=[0,0,...,0]$, and calculate the log-likelihood of the parameters at $t$ iteration with:\n",
    "\n",
    "$$L(w^{(t)}) = \\sum \\limits _{j=1} ^{N} y_j(x_j w^{(t)}) -ln(1 + exp(x_jw^{(t)})$$\n",
    "\n",
    "This tells us how good we are doing at iteration $t$. We want to maximize the likelihood (make it smaller), so we compute $w^{(t)}$ for many iterations, and when we donâ€™t see improvements on the $L(w^{(t)})$ we stop.  \n",
    "\n",
    "The `derivative_func` function calculates the derivative to be used in the update function of the parameters vector ($w$). The `update_w` function updates the vector $w$ of parameters based on the derivative at each iteration:  \n",
    "\n",
    "$$w^{(t+1)} = w^{(t)} - \\eta \\lambda w^{(t)} + \\frac{\\eta}{N} \\frac{\\partial L}{\\partial w}$$\n",
    "\n",
    "There are two important constants to consider: $\\eta$ and $\\lambda$. Both will affect the convergence of the algorithm and we should play around a little to choose good values. We are going to use a learning rate $\\eta$ such that the log-likelihood converges in about 300 iterations and is monotonically increasing. And shrinkage ($\\lambda$) of 0.0001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8bdf30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########Functions used ################\n",
    "#Likelihood function\n",
    "def likelihood(wt,x,y):\n",
    "    L=0\n",
    "    for j in range(N):\n",
    "        L+=y[j]*np.dot(x[j,],wt) - math.log(1 + math.exp(np.dot(x[j,],wt)))\n",
    "    #print(np.dot(x[j,],wt))\n",
    "    return(-1*L)\n",
    "\n",
    "#derivative function\n",
    "def derivative_func(wt,x,y):\n",
    "    derivative = [0]*p\n",
    "    for j in range(N):\n",
    "        dot_product=np.dot(x[j,],wt)\n",
    "        parenthesis=y[j] - math.exp(dot_product)/(1 + math.exp(dot_product))\n",
    "        for k in range(p):\n",
    "            derivative[k] += x[j,k] * parenthesis\n",
    "    return(derivative)\n",
    "\n",
    "#update w function\n",
    "def update_w(wt,derivative):\n",
    "    derivative_term = [i * (n/N) for i in derivative]\n",
    "    wt_next = wt - n*l*wt + derivative_term\n",
    "    return(wt_next)\n",
    "    \n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d4b914",
   "metadata": {},
   "source": [
    "***\n",
    "### Defining parmeters we know ###\n",
    "\n",
    "As we can see from the function definitions above we need some parameters to be able to start our algorithm. We need an initial $w$ vector (called wt in the functions), that we initialize as a vector of zeros. We also need to know the number of observations $N$ and features $p$, and we can take that from the `.shape` attribute of the dataset. We also need to know the value of $\\lambda$ (called l, for lambda, in the functions), that we determined to be 0.0001 here. The value of $\\eta$ (called n in the functions), that we estimate by runing the algorithm a few times with 300 iterations and looking at the convergence plot of the log-likelihood through the iterations as depicted further down. We also need the number of iterations that we want to run the agorotihm for. For this dataset 300 seem to work well.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61bb5fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the parameters we know:\n",
      "Number of observations (N): 6000, \n",
      "Number of Features (p): 5001, \n",
      "Number of iterations: 300, \n",
      "Shrinkage (l): 0.0001, \n",
      "Learning Rate (n): 0.2\n"
     ]
    }
   ],
   "source": [
    "#defining the parameters needed\n",
    "#lambda\n",
    "l=0.0001\n",
    "#Number of observations N and features p\n",
    "N, p =x.shape\n",
    "#Eta\n",
    "n=0.2\n",
    "#Number of iterations\n",
    "iterations=300\n",
    "new_line = \"\\n\"\n",
    "print(f\"These are the parameters we know:{new_line}Number of observations (N): {N}, {new_line}Number of Features (p): {p}, {new_line}Number of iterations: {iterations}, {new_line}Shrinkage (l): {l}, {new_line}Learning Rate (n): {n}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5686a84f",
   "metadata": {},
   "source": [
    "***\n",
    "### Runing the Algorithm ###\n",
    "\n",
    "Now that we have defined our functions and constants we need to get our agorithm to run a few iterations to get a good estimate of the parameters. We are fitting (or training) the model to our training data and estimating the coeficient vector $w$ that best fits our data to be used for our predictions. Below we run the algorithm for a specific number of iterations and print the iteration number and log-likelihood of the paramaters every tenth iteration. We stop when we do not see much improvement. Be patient, or get the number of iterations to be small (it might not converge though) because this takes a few minutes to run (~38, with my puny laptop)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "595d62a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration = 0, L = 4158.883083360076\n",
      "iteration = 10, L = 392.25249531334714\n",
      "iteration = 20, L = 272.19416558135566\n",
      "iteration = 30, L = 208.63158514942185\n",
      "iteration = 40, L = 168.35048743777497\n",
      "iteration = 50, L = 140.69403542941785\n",
      "iteration = 60, L = 120.72412056960923\n",
      "iteration = 70, L = 105.69151257350022\n",
      "iteration = 80, L = 93.9885566546067\n",
      "iteration = 90, L = 84.62618832208136\n",
      "iteration = 100, L = 76.96759486303631\n",
      "iteration = 110, L = 70.58670112099398\n",
      "iteration = 120, L = 65.18840314180301\n",
      "iteration = 130, L = 60.561985394228834\n",
      "iteration = 140, L = 56.55298097176521\n",
      "iteration = 150, L = 53.04556910345991\n",
      "iteration = 160, L = 49.95119419784829\n",
      "iteration = 170, L = 47.20098558641116\n",
      "iteration = 180, L = 44.74057463451263\n",
      "iteration = 190, L = 42.526468298593905\n",
      "iteration = 200, L = 40.52345942960001\n",
      "iteration = 210, L = 38.702743565418324\n",
      "iteration = 220, L = 37.04052704152707\n",
      "iteration = 230, L = 35.51698306555399\n",
      "iteration = 240, L = 34.11545831255669\n",
      "iteration = 250, L = 32.82186259424604\n",
      "iteration = 260, L = 31.62419414368017\n",
      "iteration = 270, L = 30.512166616857996\n",
      "iteration = 280, L = 29.476913263121105\n",
      "iteration = 290, L = 28.51075026135134\n",
      "Duration: 0:37:55.429315\n"
     ]
    }
   ],
   "source": [
    "#time stamp\n",
    "start_time = datetime.now()\n",
    "\n",
    "#Runing the algorithm\n",
    "#Initializing vector w at time zero\n",
    "W=np.zeros((p,), dtype=int)\n",
    "#Initialize a list to store the log-likelihood values\n",
    "L=[]\n",
    "#run the algorithm for a specific number of iterations\n",
    "for t in range(iterations):\n",
    "    Lt=likelihood(W,x,y)\n",
    "    D = derivative_func(W,x,y)\n",
    "    W =update_w(W,D)\n",
    "    L.append(Lt)\n",
    "    if t % 10 == 0: \n",
    "        print(\"iteration = {}, L = {}\".format(t, Lt))\n",
    "\n",
    "#more time stamp\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85210df6",
   "metadata": {},
   "source": [
    "***\n",
    "### Plot of the Log-Likelihood at each iteration ###\n",
    "\n",
    "To be able to see if our model has converged we need to plot the Log-likelihood at each iteration and inspect the plot for a plateau (where we do not see much improvement). The code below uses `pyplot` to achieve this in a simple way. We also save the plot in a `.jpeg` file for further use. When finding a good learning rate ($\\eta$) I ran the agorithm several times with different values of $\\eta$ and chose the one that performed best in 300 iterations. We can play around with both $\\eta$ and $\\lambda$ to improve convergence of the mdoel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aa9391a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#preparing the iteration to look good as X label for the plot (from 1 to 300)\n",
    "it=list(range(1,(iterations+1)))\n",
    "#preparing to plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.autoscale()\n",
    "plt.title(\"Log-likelihood at each iteration - 0.2 Learning Rate\")\n",
    "plt.xlabel(\"Iteration number\")\n",
    "plt.ylabel(\"-1(Log-likelihood)\")\n",
    "plt.plot(it,L)\n",
    "#ploting here in the notebook\n",
    "plt.show()\n",
    "#saving the plot to a .jpeg file\n",
    "fig.savefig(\"plot.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc259f2a",
   "metadata": {},
   "source": [
    "***\n",
    "### Making predictions ###\n",
    "\n",
    "Now that we have the best parameters ($w$) for our data we can use this vector to make classification predictions of our datasets. We can use both train and test sets, but be mindful that we estimated these parameters using the train set and therefore should expect the error estimates on that set to be smaller, if not zero! The real value of our model is evaluated with small error estimates on the test set, or unseen data!\n",
    "\n",
    "We make predictions and then calculate the misclassification error using `sklearn` accuracy_score function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e3281d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification error for train data is 0.0, and for test data is 0.028000000000000025\n"
     ]
    }
   ],
   "source": [
    "#Predicting and geting misclassification errors for train and test datasets\n",
    "#Train set: x, y\n",
    "#creating a list to store the predictions\n",
    "y_predict = []\n",
    "#transforming our prediction probabilities to 0 and 1\n",
    "for row in x:\n",
    "    score=np.dot(W,row)\n",
    "    prob=math.exp(score) / 1+math.exp(score) #probability that observation in row is 1\n",
    "    if (prob < 0.5):\n",
    "        y_predict.append(0)\n",
    "    else:\n",
    "        y_predict.append(1)\n",
    "#calculating the misclassification error as 1 - accuracy score\n",
    "error = 1 - accuracy_score(y,y_predict)\n",
    "    \n",
    "#Test set: xtest, ytest\n",
    "ytest_predict = []\n",
    "for row in xtest:\n",
    "    score=np.dot(W,row)\n",
    "    prob=math.exp(score) / 1+math.exp(score) #probability that observation in row is 1\n",
    "    if (prob < 0.5):\n",
    "        ytest_predict.append(0)\n",
    "    else:\n",
    "        ytest_predict.append(1)\n",
    "\n",
    "error_test = 1 - accuracy_score(ytest,ytest_predict)\n",
    "\n",
    "print(\"Misclassification error for train data is {}, and for test data is {}\".format(error, error_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41badcf",
   "metadata": {},
   "source": [
    "***\n",
    "## Using the Logit library ##\n",
    "\n",
    "In this folder there is a file called `LogitFunctions.py` that cotains all the code above inside defined functions. We can use this file to run the Logistic Regression algorithm as shown below. We just need to import the Module/library as we do so many others and use the functions. There is already some difference from the functions above to the definitions in the file, since to run as a library all of the functions needed to work together, but all calculations are the same. Here we need only call the `logit_algorithm` function with the parameters needed:  \n",
    "\n",
    "- $X$ data matrix, \n",
    "- $y$ vector of labels, \n",
    "- $\\eta$, \n",
    "- $\\lambda$ and \n",
    "- number of iterations we want.  \n",
    "\n",
    "To see how well our model works on unseen data we just need to call the `y_predict` function with the following parameters:  \n",
    "- $X$ test data matrix, \n",
    "- $y$ test label vector and \n",
    "- the $w$ vector of coeficients that we get from our algorithm function.  \n",
    "\n",
    "The algorithm function also returns a list with all the log-likelihood scores from each iteration step that we can use to make a plot as we did before.  \n",
    "\n",
    "One interesting feature of jupyter is that we can always start writing the name of the function we want and use `tab` to autocomplete it (after importing the module). And we can use `shift-tab` to get the function's docstrings, if we do that for `logit_algorithm` we can see which arguments it takes and if we click on the `+` button we can see everything. It is always good to be sure what are the values the function returns and plan your variables accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c662716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration = 0, L = 4158.883083360076\n",
      "iteration = 10, L = 298.1320360493172\n",
      "iteration = 20, L = 191.83915775116174\n",
      "iteration = 30, L = 142.18581498288688\n",
      "iteration = 40, L = 112.90963140039368\n",
      "iteration = 50, L = 93.8669569368013\n",
      "iteration = 60, L = 80.43637228767415\n",
      "iteration = 70, L = 70.41814059457396\n",
      "iteration = 80, L = 62.6437023493224\n",
      "iteration = 90, L = 56.430081641771466\n",
      "iteration = 100, L = 51.348315455384544\n",
      "iteration = 110, L = 47.11425612162519\n",
      "iteration = 120, L = 43.53182429557356\n",
      "iteration = 130, L = 40.46120401742599\n",
      "iteration = 140, L = 37.79996030160878\n",
      "iteration = 150, L = 35.47130764061242\n",
      "iteration = 160, L = 33.416545371175886\n",
      "iteration = 170, L = 31.590025148835135\n",
      "iteration = 180, L = 29.95571211119874\n",
      "iteration = 190, L = 28.484779667544508\n",
      "iteration = 200, L = 27.153892394439822\n",
      "iteration = 210, L = 25.943957655915025\n",
      "iteration = 220, L = 24.83920308960691\n",
      "iteration = 230, L = 23.826484817954526\n",
      "iteration = 240, L = 22.894761734555406\n",
      "iteration = 250, L = 22.0346911301482\n",
      "iteration = 260, L = 21.23831418847323\n",
      "iteration = 270, L = 20.498808879015872\n",
      "iteration = 280, L = 19.81029397578064\n",
      "iteration = 290, L = 19.16767227150571\n",
      "iteration = 300, L = 18.5665041362658\n",
      "iteration = 310, L = 18.00290478250899\n",
      "iteration = 320, L = 17.47346020792719\n",
      "iteration = 330, L = 16.975157970827116\n",
      "iteration = 340, L = 16.505329831623495\n",
      "iteration = 350, L = 16.061603953268325\n",
      "iteration = 360, L = 15.64186485227827\n",
      "iteration = 370, L = 15.244219672706121\n",
      "iteration = 380, L = 14.866969648248826\n",
      "iteration = 390, L = 14.508585844630286\n",
      "iteration = 400, L = 14.167688451534447\n",
      "iteration = 410, L = 13.843029032539873\n",
      "iteration = 420, L = 13.533475251557432\n",
      "iteration = 430, L = 13.237997681810738\n",
      "iteration = 440, L = 12.955658373434119\n",
      "iteration = 450, L = 12.685600912092822\n",
      "iteration = 460, L = 12.427041746573886\n",
      "iteration = 470, L = 12.179262600307023\n",
      "iteration = 480, L = 11.941603811977721\n",
      "iteration = 490, L = 11.71345847517188\n",
      "iteration = 500, L = 11.494267267389173\n",
      "iteration = 510, L = 11.283513875633698\n",
      "iteration = 520, L = 11.080720939794837\n",
      "iteration = 530, L = 10.885446446703341\n",
      "iteration = 540, L = 10.69728051750883\n",
      "iteration = 550, L = 10.515842539219054\n",
      "iteration = 560, L = 10.340778598137499\n",
      "iteration = 570, L = 10.171759178767275\n",
      "iteration = 580, L = 10.008477096690969\n",
      "iteration = 590, L = 9.850645638127052\n",
      "iteration = 600, L = 9.697996882452754\n",
      "iteration = 610, L = 9.550280187035602\n",
      "iteration = 620, L = 9.407260816342204\n",
      "iteration = 630, L = 9.268718699545095\n",
      "iteration = 640, L = 9.134447302792157\n",
      "iteration = 650, L = 9.004252603976816\n",
      "iteration = 660, L = 8.877952159303591\n",
      "iteration = 670, L = 8.755374252198775\n",
      "iteration = 680, L = 8.636357116214555\n",
      "iteration = 690, L = 8.520748224527617\n",
      "iteration = 700, L = 8.408403639467938\n",
      "iteration = 710, L = 8.299187416241784\n",
      "iteration = 720, L = 8.192971055652848\n",
      "iteration = 730, L = 8.089633001188654\n",
      "iteration = 740, L = 7.989058176329582\n",
      "iteration = 750, L = 7.891137558378998\n",
      "iteration = 760, L = 7.7957677854926315\n",
      "iteration = 770, L = 7.702850793930037\n",
      "iteration = 780, L = 7.612293482849575\n",
      "iteration = 790, L = 7.524007404237692\n",
      "iteration = 800, L = 7.4379084758004606\n",
      "iteration = 810, L = 7.3539167148580304\n",
      "iteration = 820, L = 7.271955991470163\n",
      "iteration = 830, L = 7.191953799191973\n",
      "iteration = 840, L = 7.113841042006127\n",
      "iteration = 850, L = 7.0375518361170775\n",
      "iteration = 860, L = 6.96302332541068\n",
      "iteration = 870, L = 6.890195509492638\n",
      "iteration = 880, L = 6.819011083316569\n",
      "iteration = 890, L = 6.749415287500595\n",
      "iteration = 900, L = 6.681355768510155\n",
      "iteration = 910, L = 6.614782447956896\n",
      "iteration = 920, L = 6.549647400327498\n",
      "iteration = 930, L = 6.4859047385140185\n",
      "iteration = 940, L = 6.423510506573213\n",
      "iteration = 950, L = 6.362422579184202\n",
      "iteration = 960, L = 6.302600567325949\n",
      "iteration = 970, L = 6.2440057297252585\n",
      "iteration = 980, L = 6.186600889671714\n",
      "iteration = 990, L = 6.130350356821373\n",
      "Misclassification error for train data is 0.0, and for test data is 0.025000000000000022\n"
     ]
    }
   ],
   "source": [
    "import LogitFunctions as logit\n",
    "\n",
    "L, W = logit.logit_algorithm(x,y,0.3,0.0001,1000)\n",
    "error = logit.y_predict(y,x,W)\n",
    "error_test = logit.y_predict(ytest,xtest,W)\n",
    "\n",
    "print(\"Misclassification error for train data is {}, and for test data is {}\".format(error, error_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "847e6f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#preparing the iteration to look good as X label for the plot (from 1 to 300)\n",
    "it=list(range(1,(1000+1)))\n",
    "#preparing to plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.autoscale()\n",
    "plt.title(\"Log-likelihood at each iteration - 0.3 Learning Rate\")\n",
    "plt.xlabel(\"Iteration number\")\n",
    "plt.ylabel(\"-1(Log-likelihood)\")\n",
    "plt.plot(it,L)\n",
    "#ploting here in the notebook\n",
    "plt.show()\n",
    "#saving the plot to a .jpeg file\n",
    "fig.savefig(\"plot2.jpeg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
